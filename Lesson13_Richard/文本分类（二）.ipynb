{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前面内容中，我们介绍了文本分类的基本流程和词袋模型，以及利用词袋模型对电影评论数据进行分类的案列。此部分，我们主要介绍主题模型在文本分类中的应用。 在主题模型中，主题表示一个概念、一个方面，表现为一系列相关的单词，是这些单词的条件概率。形象来说，主题就是一个桶，里面装了出现概率较高的单词，这些单词与这个主题有很强的相关性【引用自[CSDN博客](http://blog.csdn.net/huagong_adu/article/details/7937616)】。具体的理论阐述可参考[CSDN](http://blog.csdn.net/huagong_adu/article/details/7937616)和[知乎](https://www.zhihu.com/search?type=content&q=%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本示例中，**我们主要介绍主题模型的基本思想和应用方法。**在前面的词袋模型中，我们将每篇文本直接表示成若干单词的频率向量，如：\n",
    "\n",
    "| 句子 | I | like | he | likes | dogs | too| very | much\n",
    "| - | :-: |  -: |  -: | -: |-: |-: |-: |-: |\n",
    "| S1 | 1 | 1 | 0 | 0 | 1 | 0 | 1 | 1|\n",
    "|S2 | 0 | 0 | 1 | 1 | 1 | 1 | 0 | 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在主题模型中，我们在文档和单词之间还引入了主题变量，也就是每篇文本我们可以看成是由若干主题组合而成，而各主题又拥有不同概率出现的单词。主题是一种隐变量，给我们任意一篇文章，我们只能看到文章以及里面的单词，看不到主题。但是可以建立文章和单词的矩阵，比如词袋模型中的频率矩阵，进行一定矩阵分解来获取主题，最后我们使用主题来表示文本。简单的可以将文档-主题，主题-单词用表格示意如下：\n",
    "\n",
    "             文档-主题\n",
    "\n",
    "| 句子 | 主题1 | 主题2 | 主题3 | 主题4 | \n",
    "| - | :-: |  -: |  -: | -: |\n",
    "|文档1 | 1 | 1 | 0 | 0 | \n",
    "|文档2 | 0 | 0 | 1 | 1 | \n",
    "\n",
    "              主题-单词\n",
    "\n",
    "| 主题 | 单词1 | 单词2 | 单词3 | 单词4 | 单词...\n",
    "| - | :-: |  -: |  -: | -: |\n",
    "|主题1 | 0 | 1 | 0 | 0 | ...|\n",
    "|主题2 | 0 | 0 | 1 | 1 | ...|\n",
    "|主题3 | 1 | 0 | 0 | 0 | ...|\n",
    "|主题4 | 0 | 1 | 1 | 1 | ...|\n",
    "\n",
    "我们可以把文章表示成主题以一定概率的组合而成的向量，而主题又可以表示成单词组合而成的向量，文档单词之间的条件概率可以分解成：\n",
    "![topicmodel](http://img.my.csdn.net/uploads/201209/03/1346651721_6594.PNG)\n",
    "\n",
    "图示方法：\n",
    "![topicmodel](http://img.my.csdn.net/uploads/201209/03/1346651772_3109.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们得到了文本的主题表示矩阵，可以利用主题向量进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.文本数据预处理\n",
    "电影评论数据可以直接通过nltk工具包直接获取，里面可以读取每篇评论内容、感情色彩以及单词。部分代码参考了CSDN博客http://blog.csdn.net/sinat_20791575/article/details/58661827 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = movie_reviews.categories()\n",
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "电影评论从感情色彩上分为积极与负面两种类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评论数量： 2000\n"
     ]
    }
   ],
   "source": [
    "print('评论数量：', len(movie_reviews.fileids()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "积极评论数量： 1000\n",
      "负面评论数量： 1000\n"
     ]
    }
   ],
   "source": [
    "print('积极评论数量：', len(movie_reviews.fileids('pos')))\n",
    "print('负面评论数量：', len(movie_reviews.fileids('neg')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "两种类型的数据是均衡的。下面我们查看第一个文件数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'plot : two teen couples go to a church party , drink and then drive . \\nthey get into an accident . \\none of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \\nwhat\\'s the deal ? \\nwatch the movie and \" sorta \" find out . . . \\ncritique : a mind-fuck movie for the teen generation that touches on a very cool idea , but presents it in a very bad package . \\nwhich i'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first = movie_reviews.fileids()[0]\n",
    "movie_reviews.open(first).read()[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到当前文档除了单词之外还有标点符号，数字换行符等。在后面的处理中，我们可以将这些标点符号看成是没有意义的去除掉，也可以保留下来作为一个符号进行分析，特别是问号、感叹号也包含一定感情色彩。\n",
    "\n",
    "**注意，一般读取的文档内容都是句子或者段落的长文本，需要进行分词。**\n",
    "\n",
    "这里我们保留所有的符号，下面我们通过nltk工具获取每篇文章的单词以及对应的情感标签。nltk已经对文本进行了分词，但是为了演示完整流程，我们使用原始的文本，将原始的长文本分成单词和标点符号序列；分词时还可以分成2元组、3元组(n-gram)，比如“machine learning\"、“a good dog”。对于中文稍微有些复杂，因为没有空格来区分词组，比如“今天我买了一件衣服”，每个汉字可以当做一个词，但是根据常识应该分为“今天”、“我”、“买了”、“一件”、“衣服”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#读取每篇评论及其对应的标签\n",
    "document_label_pairs = [(movie_reviews.raw(fileid), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#随机打乱次序\n",
    "np.random.shuffle(document_label_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents, labels = list(zip(*document_label_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.文本清洗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本清洗可以看成是数据预处理。一般有两项工作：\n",
    "- 去除无意义符号。文本一般还有文字、标点符号、空格、数字，甚至有很多缩写、外文、错误的拼写，有的时候我们只需要处理单纯的文本比如单词或汉字，其他符号可以当做噪声去除掉。\n",
    "- 对文本进行分词。将文本内容分成词组等形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面我们已经利用nltk工具直接获得了每篇评论的单词序列，这里再自己做一遍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果原始文件是一个字符串，比如我们将上文看作一个长字符串，并且需要去掉标点符号，我们可以使用Python工具完成。\n",
    "\n",
    "### 2.1文本清洗示例\n",
    "可以正则表达式来去掉标点符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'according to the publicity material  with this movie the directors \" hope to restore good oldfashioned bowling to its rightful place in the mainstream of american consciousness  \" \\nhmm  \\nyou never know  they just might be on to something  \\nwhat with the rise of geek chic  lounge music and seventies fashion  the evercontrary kids of the nineties might just latch on to bowling as another terminally '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#正则表达，去除表达符号\n",
    "import re\n",
    "text = document_label_pairs[0][0]\n",
    "#指定需要去掉的标点符号\n",
    "text = re.sub('[,.!:;#()-]', '', text)\n",
    "text[:400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['according', 'to', 'the', 'publicity', 'material', 'with', 'this', 'movie', 'the', 'directors', '\"', 'hope', 'to', 'restore', 'good', 'oldfashioned', 'bowling', 'to', 'its', 'rightful']\n"
     ]
    }
   ],
   "source": [
    "#分词\n",
    "print(text.split()[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2对电影评论数据进行文本清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#去掉标点符号\n",
    "documents = [re.sub('[_,.!:;#()0-9]', '', text) for text in documents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3训练集和测试集划分\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "doc_train, doc_test, label_train, label_test = train_test_split(documents, labels, test_size=0.3, random_state=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.文本主题模型表达\n",
    "\n",
    "文本数据特点是字符多、长度不一致，提取特征比一般的数据复杂。文本表达大致有三个方向，具体可参考[CSDN博客](http://blog.csdn.net/wangongxi/article/details/51591031)：\n",
    "- 传统上的模型有词袋模型，也就是将文本看成单词的集合，不考虑单词的次序，比如\"I love this dog\"可以表达成[I, love, this, dog]，但是这样依然是文字符号，人能理解，计算机不能理解，所以需要转换成向量等形式以作为计算机的输入，比如将句子表示成词库中每个单词的频率组成的向量。通常文本库中的单词量很大，数万、数十万，很多单词只在部分文本中出现，所以文本表示的矩阵是维数很高的稀疏矩阵。\n",
    "- 90年代有学者提出了[主题模型](http://blog.csdn.net/huagong_adu/article/details/7937616)，也就是将文档看成是若干主题的组合，比如一篇文章既讨论政治又讨论经济，而主题又是单词的组合。比较主流的有LSA方法，LDA方法等。\n",
    "![Topic Model](http://img.my.csdn.net/uploads/201209/03/1346651772_3109.PNG)\n",
    "- 近年来，随着深度学习的雄起，词嵌入（word embedding), 句子嵌入（sentence embedding）等概念和方法在文本处理中取得了突飞猛进。其理念是使用嵌入低维向量来表示单词、句子甚至文档，意义相近的单词距离较近，同理，意义相近的句子或文档向量距离也比较近。\n",
    "\n",
    "**本次主要讨论主题模型**。\n",
    "\n",
    "首先要通过词袋模型建立文档单词矩阵：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.1电影评论频率特征提取\n",
    "利用sklearn工具包进行特征提取，去掉连接词，最低保留单词频率为2。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=2, stop_words='english', ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(doc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_train_vec = vectorizer.transform(doc_train)\n",
    "doc_test_vec = vectorizer.transform(doc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Shape: (1400, 19743)\n",
      "Testing data Shape: (600, 19743)\n"
     ]
    }
   ],
   "source": [
    "print('Training data Shape:', doc_train_vec.shape)\n",
    "print('Testing data Shape:', doc_test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.DataFrame(doc_train_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aardman</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abandons</th>\n",
       "      <th>abby</th>\n",
       "      <th>abc</th>\n",
       "      <th>abducted</th>\n",
       "      <th>...</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zoolander</th>\n",
       "      <th>zoologist</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zooming</th>\n",
       "      <th>zooms</th>\n",
       "      <th>zoot</th>\n",
       "      <th>zorro</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zwick</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 19743 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      aa  aardman  aaron  abandon  abandoned  abandonment  abandons  abby  \\\n",
       "doc1   0        0      0        0          0            0         0     0   \n",
       "doc2   0        0      0        0          0            0         0     0   \n",
       "doc3   0        0      0        0          0            0         0     0   \n",
       "doc4   0        0      0        0          0            0         0     0   \n",
       "doc5   0        0      0        0          0            0         0     0   \n",
       "\n",
       "      abc  abducted  ...    zoo  zoolander  zoologist  zoom  zooming  zooms  \\\n",
       "doc1    0         0  ...      0          0          0     0        0      0   \n",
       "doc2    0         0  ...      0          0          0     0        0      0   \n",
       "doc3    0         0  ...      0          0          0     0        0      0   \n",
       "doc4    0         0  ...      0          0          0     0        0      0   \n",
       "doc5    0         0  ...      0          0          0     0        0      0   \n",
       "\n",
       "      zoot  zorro  zucker  zwick  \n",
       "doc1     0      0       0      0  \n",
       "doc2     0      0       0      0  \n",
       "doc3     0      0       0      0  \n",
       "doc4     0      0       0      0  \n",
       "doc5     0      0       0      0  \n",
       "\n",
       "[5 rows x 19743 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns = vectorizer.get_feature_names()\n",
    "data.index = ['doc'+str(i+1) for i in range(len(data))]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是个很庞大的稀疏矩阵，所以sklearn将之存储到稀疏矩阵里。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2获取主题变量\n",
    "我们尝试使用“Non-negative Matrix Factorization”和“Latent Dirichlet Allocation”两种方法进行主题提取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMF done in 11.055s.\n",
      "LDA done in 7.299s.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from time import time\n",
    "n_components = 30\n",
    "# Fit the NMF model, suppose there are 10 topics\n",
    "\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5)\n",
    "lda = LatentDirichletAllocation(n_components =n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "nmf.fit(doc_train_vec)\n",
    "print(\"NMF done in %0.3fs.\" % (time() - t0))\n",
    "t0 = time()\n",
    "lda.fit(doc_train_vec)\n",
    "print(\"LDA done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以查看NMF方法提取到的每个主题里面包含哪些单词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "The 0th component:\n",
      "['fact', 'point', 'performance', 'scenes', 'really', 'plot', 'great', 'seen', 'films', 'film']\n",
      "********************\n",
      "The 1th component:\n",
      "['watch', 'hollywood', 'interesting', 'think', 'seen', 've', 'minutes', 'plot', 'movies', 'movie']\n",
      "********************\n",
      "The 2th component:\n",
      "['father', 'old', 'dog', 'year', 'love', 'time', 'world', 'family', 'man', 'life']\n",
      "********************\n",
      "The 3th component:\n",
      "['times', 'things', 'harry', 'years', 'make', 'stories', 'doesn', 'love', 'characters', 'story']\n",
      "********************\n",
      "The 4th component:\n",
      "['trilogy', 'characters', 'original', 'lucas', 'jar', 'jedi', 'phantom', 'menace', 'wars', 'star']\n",
      "********************\n",
      "The 5th component:\n",
      "['fflewdurr', 'studio', 'video', 'film', 'disney', 'hen', 'wen', 'king', 'black', 'cauldron']\n",
      "********************\n",
      "The 6th component:\n",
      "['course', 'series', 'planet', 'species', 'quite', 'films', 'fincher', 'ripley', 'aliens', 'alien']\n",
      "********************\n",
      "The 7th component:\n",
      "['arnold', 'new', 'end', 'man', 'script', 'director', 'films', 'scenes', 'plot', 'action']\n",
      "********************\n",
      "The 8th component:\n",
      "['characters', 'film', 'original', 'sequel', 'films', 'williamson', 'sidney', 'killer', 'horror', 'scream']\n",
      "********************\n",
      "The 9th component:\n",
      "['comedy', 'guy', 'friend', 'time', 'performance', 'audience', 'role', 'best', 'characters', 'character']\n",
      "********************\n",
      "The 10th component:\n",
      "['mary', 'work', 'doesn', 'charlie', 'brothers', 'farrellys', 'carrey', 'irene', 'comedy', 'funny']\n",
      "********************\n",
      "The 11th component:\n",
      "['mulder', 'film', 'movie', 'cinema', 'characters', 'fans', 'carter', 'files', 'television', 'series']\n",
      "********************\n",
      "The 12th component:\n",
      "['death', 'rock', 'world', 'films', 'looks', 'kind', 'really', 'way', 'people', 'like']\n",
      "********************\n",
      "The 13th component:\n",
      "['going', 'did', 'say', 've', 'think', 'don', 'way', 'really', 'know', 'just']\n",
      "********************\n",
      "The 14th component:\n",
      "['just', 'character', 'job', 'gets', 'time', 'really', 'performance', 'best', 'great', 'good']\n",
      "********************\n",
      "The 15th component:\n",
      "['mr', 'grier', 'films', 'pulp', 'fiction', 'brown', 'chan', 'ordell', 'tarantino', 'jackie']\n",
      "********************\n",
      "The 16th component:\n",
      "['world', 'battle', 'saving', 'hanks', 'soldiers', 'spielberg', 'private', 'men', 'ryan', 'war']\n",
      "********************\n",
      "The 17th component:\n",
      "['angels', 'human', 'dark', 'best', 'world', 'original', 'new', 'pig', 'babe', 'city']\n",
      "********************\n",
      "The 18th component:\n",
      "['mulan', 'original', 'jane', 'family', 'voice', 'apes', 'animation', 'animated', 'tarzan', 'disney']\n",
      "********************\n",
      "The 19th component:\n",
      "['doesn', 'time', 'bowling', 'house', 'walter', 'godzilla', 'momma', 'dude', 'lebowski', 'big']\n",
      "********************\n",
      "The 20th component:\n",
      "['home', 'new', 'disc', 'dvd', 'rocky', 'original', 'horror', 'version', 'film', 'time']\n",
      "********************\n",
      "The 21th component:\n",
      "['beatrice', 'game', 'malkovich', 'brooks', 'frank', 'don', 'mother', 'does', 'time', 'john']\n",
      "********************\n",
      "The 22th component:\n",
      "['does', 'winslet', 'dicaprio', 'jack', 'effects', 'rose', 'love', 'cameron', 'ship', 'titanic']\n",
      "********************\n",
      "The 23th component:\n",
      "['effects', 'death', 'jill', 'mighty', 'simon', 'ryan', 'meg', 'gorilla', 'young', 'joe']\n",
      "********************\n",
      "The 24th component:\n",
      "['special', 'rico', 'film', 'effects', 'fi', 'sci', 'verhoeven', 'star', 'starship', 'troopers']\n",
      "********************\n",
      "The 25th component:\n",
      "['girls', 'woman', 'car', 'director', 'love', 'scenes', 'sex', 'people', 'does', 'scene']\n",
      "********************\n",
      "The 26th component:\n",
      "['funny', 'dogma', 'jokes', 'west', 'wild', 'silent', 'jay', 'bob', 'kevin', 'smith']\n",
      "********************\n",
      "The 27th component:\n",
      "['doesn', 'like', 'great', 'films', 'chickens', 'park', 'seen', 've', 'run', 'chicken']\n",
      "********************\n",
      "The 28th component:\n",
      "['cops', 'like', 'time', 'white', 'american', 'peebles', 'sweet', 'van', 'black', 'sweetback']\n",
      "********************\n",
      "The 29th component:\n",
      "['know', 'acting', 'really', 'plot', 'make', 'don', 'doesn', 'time', 'guy', 'bad']\n"
     ]
    }
   ],
   "source": [
    "for i, component in enumerate(nmf.components_):\n",
    "    #The most important 20 words' index\n",
    "    nlargest = component.argsort()[-10:]\n",
    "    word_list = []\n",
    "    for n in nlargest:\n",
    "        word_list.append(feature_words[n])\n",
    "    print('*'*20)\n",
    "    print('The ' + str(i) + 'th component:')\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA方法提取的主题，有些主题里面单词比较相关。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "The 0th component:\n",
      "['daryl', 'private', 'investigator', 'film', 'arlo', 'pullman', 'gloria', 'stark', 'kasdan', 'zero']\n",
      "********************\n",
      "The 1th component:\n",
      "['scene', 'trek', 'love', 'story', 'violins', 'time', 'just', 'movie', 'like', 'film']\n",
      "********************\n",
      "The 2th component:\n",
      "['platitudes', 'cropped', 'overseen', 'hallucinatory', 'arc', 'worldview', 'wail', 'banshee', 'joan', 'besson']\n",
      "********************\n",
      "The 3th component:\n",
      "['sexual', 'harry', 'character', 'time', 'just', 'like', 'movie', 'brody', 'film', 'ritchie']\n",
      "********************\n",
      "The 4th component:\n",
      "['french', 'rodman', 'close', 'stavros', 'movie', 'cameron', 'humans', 'world', 'terminator', 'film']\n",
      "********************\n",
      "The 5th component:\n",
      "['way', 'characters', 'character', 'story', 'good', 'time', 'just', 'like', 'movie', 'film']\n",
      "********************\n",
      "The 6th component:\n",
      "['film', 'surgery', 'troi', 'frakes', 'riker', 'irwin', 'trek', 'awakenings', 'borg', 'virgil']\n",
      "********************\n",
      "The 7th component:\n",
      "['elliott', 'vinnie', 'felicity', 'cars', 'raines', 'redefines', 'duke', 'rohmer', 'kip', 'memphis']\n",
      "********************\n",
      "The 8th component:\n",
      "['albertson', 'gwen', 'ballroom', 'fingered', 'film', 'buttons', 'borgnine', 'winters', 'poseidon', 'kiki']\n",
      "********************\n",
      "The 9th component:\n",
      "['max', 'andre', 'monastery', 'quinlan', 'serena', 'lilianna', 'romania', 'gheorghe', 'muresan', 'sammy']\n",
      "********************\n",
      "The 10th component:\n",
      "['teri', 'movie', 'cajun', 'like', 'sabotaged', 'alec', 'abusing', 'hatcher', 'butterfly', 'film']\n",
      "********************\n",
      "The 11th component:\n",
      "['depardieu', 'dalmatian', 'dixon', 'schroeder', 'gingerbread', 'magruder', 'sunny', 'dalmatians', 'von', 'altman']\n",
      "********************\n",
      "The 12th component:\n",
      "['film', 'fiorentino', 'serendipity', 'metatron', 'loki', 'bartleby', 'disclaimer', 'dogmatic', 'bethany', 'dogma']\n",
      "********************\n",
      "The 13th component:\n",
      "['action', 'character', 'bad', 'like', 'chan', 'good', 'city', 'movie', 'jackie', 'film']\n",
      "********************\n",
      "The 14th component:\n",
      "['grossness', 'mary', 'split', 'illness', 'shifting', 'carrey', 'farrelly', 'charlie', 'farrellys', 'irene']\n",
      "********************\n",
      "The 15th component:\n",
      "['wrestlers', 'wrestler', 'nitro', 'titus', 'king', 'bischoff', 'sinclair', 'gordie', 'wcw', 'wrestling']\n",
      "********************\n",
      "The 16th component:\n",
      "['character', 'sense', 'characters', 'time', 'frank', 'movie', 'just', 'like', 'original', 'film']\n",
      "********************\n",
      "The 17th component:\n",
      "['shot', 'robert', 'gavin', 'life', 'great', 'right', 'film', 'grisham', 'magruder', 'altman']\n",
      "********************\n",
      "The 18th component:\n",
      "['ba', 'film', 'trekkies', 'tibet', 'stiller', 'insurrection', 'stahl', 'dalai', 'lama', 'trek']\n",
      "********************\n",
      "The 19th component:\n",
      "['doesn', 'movie', 'ronna', 'time', 'good', 'character', 'story', 'shandling', 'just', 'film']\n",
      "********************\n",
      "The 20th component:\n",
      "['wrestling', 'make', 'planet', 'chocolat', 'movie', 'does', 'life', 'like', 'time', 'film']\n",
      "********************\n",
      "The 21th component:\n",
      "['goofily', 'orchestrating', 'film', 'ominously', 'homosexuals', 'transvestites', 'lugosi', 'wood', 'glen', 'glenda']\n",
      "********************\n",
      "The 22th component:\n",
      "['zippel', 'hades', 'olympian', 'ballads', 'zeus', 'shaffer', 'depart', 'meals', 'salon', 'gavin']\n",
      "********************\n",
      "The 23th component:\n",
      "['chocolat', 'shue', 'film', 'science', 'jerome', 'vincent', 'palmetto', 'gattaca', 'spawn', 'harry']\n",
      "********************\n",
      "The 24th component:\n",
      "['brady', 'rea', 'preacher', 'muppet', 'francie', 'church', 'kermit', 'duvall', 'apostle', 'sonny']\n",
      "********************\n",
      "The 25th component:\n",
      "['operative', 'tougher', 'cia', 'terrorist', 'bombers', 'arab', 'bening', 'zwick', 'terrorism', 'siege']\n",
      "********************\n",
      "The 26th component:\n",
      "['characters', 'place', 'scenes', 'reality', 'life', 'like', 'existenz', 'game', 'world', 'film']\n",
      "********************\n",
      "The 27th component:\n",
      "['doesn', 'really', 'life', 'films', 'bad', 'time', 'just', 'movie', 'like', 'film']\n",
      "********************\n",
      "The 28th component:\n",
      "['scene', 'like', 'john', 'dark', 'johnson', 'simon', 'film', 'ship', 'city', 'hackman']\n",
      "********************\n",
      "The 29th component:\n",
      "['librarian', 'hygiene', 'kroon', 'phi', 'piet', 'beta', 'sito', 'mart', 'wal', 'osmosis']\n"
     ]
    }
   ],
   "source": [
    "for i, component in enumerate(lda.components_):\n",
    "    #The most important 20 words' index\n",
    "    nlargest = component.argsort()[-10:]\n",
    "    word_list = []\n",
    "    for n in nlargest:\n",
    "        word_list.append(feature_words[n])\n",
    "    print('*'*20)\n",
    "    print('The ' + str(i) + 'th component:')\n",
    "    print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract feature for each news\n",
    "nmf_train_features = nmf.transform(doc_train_vec)\n",
    "nmf_test_features = nmf.transform(doc_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_train_features = lda.transform(doc_train_vec)\n",
    "lda_test_features = lda.transform(doc_test_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.分类\n",
    "\n",
    "这是一个监督学习里面的二元分类问题，分类的方法可以采用传统的统计机器学习方法比如决策树、朴素贝叶斯、逻辑回归、提升树等；也可以使用多层神经网络。\n",
    "\n",
    "### 4.1朴素贝叶斯\n",
    "我们首先使用经典的朴素贝叶斯方法进行分类，假设单词的条件概率都是独立的，计算每个类别单词出现的条件概率，每个文本的条件概率就是每个单词的条件概率之积。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_test(clf, train_features, test_features):\n",
    "    start = time.time()\n",
    "    #训练\n",
    "    clf.fit(train_features, label_train)\n",
    "    end = time.time()\n",
    "    print('训练时间：{:.3f}'.format(end-start))\n",
    "    #预测\n",
    "    preds = clf.predict(test_features)\n",
    "    #计算准准确率\n",
    "    #Micro F1\n",
    "    print('Micro F1: {:.3f}'.format(f1_score(label_test, preds, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间：0.003\n",
      "Micro F1: 0.680\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "train_test(clf, nmf_train_features, nmf_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间：0.002\n",
      "Micro F1: 0.497\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "train_test(clf,lda_train_features, lda_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到负面评价中有Bad， don't， doesn't等单词，而正面评价中有good, best, great, love等单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 集成方法\n",
    "还可以尝试集成方法，三个臭皮匠赛过诸葛亮。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间：0.164\n",
      "Micro F1: 0.698\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "#clf_ad = AdaBoostClassifier(n_estimators=100, learning_rate=2).fit(X_train_tfidf, y_train)\n",
    "clf_rf = RandomForestClassifier(n_estimators=50, n_jobs=4)\n",
    "train_test(clf_rf, nmf_train_features, nmf_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间：0.159\n",
      "Micro F1: 0.515\n"
     ]
    }
   ],
   "source": [
    "train_test(clf_rf, lda_train_features,lda_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用默认参数的继承方法效果并不明显。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3多层神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练时间：1.035\n",
      "Micro F1: 0.713\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(80, 20), max_iter=500, alpha=1e-4,\n",
    "                    solver='sgd', verbose=False, tol=1e-5, random_state=1,\n",
    "                    learning_rate_init=.01)\n",
    "train_test(mlp, nmf_train_features, nmf_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上去似乎主题模型分类效果不如词袋模型，很可能在矩阵分解中丢失了部分信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
