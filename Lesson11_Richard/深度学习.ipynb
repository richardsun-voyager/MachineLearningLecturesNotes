{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习的概念源于人工神经网络的研究。含多隐层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。\n",
    "\n",
    "深度学习的概念由Hinton等人于2006年提出。基于深度置信网络(DBN)提出非监督贪心逐层训练算法，为解决深层结构相关的优化难题带来希望，随后提出多层自动编码器深层结构。此外Lecun等人提出的卷积神经网络是第一个真正多层结构学习算法，它利用空间相对关系减少参数数目以提高训练性能。（摘自[百度百科](https://baike.baidu.com/item/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/3729729?fr=aladdin))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习的入门介绍资料以及课程有很多，有下列课程可供选择：\n",
    "- 优达学城的深度学习纳米学位，收费，偏向工程应用，适合理工男，使用google tensorflow完成作业。\n",
    "- Coursera上吴恩达博士的深度学习基础课程，视频免费，偏向于基础概念理论理解，作业使用numpy、tensorflow基本工具，https://www.deeplearning.ai/ 。\n",
    "- Udemy上面Kirill Eremenko入门课程，10$，讲解内容覆盖面比较全，主要使用keras练习，顺便介绍了tensorflow和pytorch，https://www.udemy.com/deeplearning/learn/v4/overview。\n",
    "- Fast.ai上面的免费深度学习课程，提供了视频和练习代码，现在介绍Pytorch编程， http://course.fast.ai/start.html。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、深度学习 VS 机器学习\n",
    "\n",
    "深度学习是机器学习研究中的一个新的领域，其动机在于建立、模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本。（摘自[深度学习世界](https://baike.baidu.com/redirect/a164WF92D-RKTc0rxLJAkTpkGBU_x8EbnZKCxFve1-u7iGLhrDFLWHSNWHlyODu31xlyoWdeW4Gu5E-zXgOXGqeKHHd4DMA)）\n",
    "\n",
    "深度机器学习方法也有监督学习与无监督学习之分，列入：\n",
    "- 监督学习，分类问题比如图像分类、文本分类等。\n",
    "- 非监督学习，自动编码器、深度置信网络以及最近比较火热的生成对抗网络等。\n",
    "\n",
    "深度学习在图像处理、自然语言处理方面取得了突破性进展，比如在ImageNet大赛中，从2012年开始的Alexnet以及后面的Vgg等深度学习模型所取得的成绩大幅度领先于其他常规模型，当前手写数字识别、人脸识别以及目标定位等技术都大量采用深度学习模型；在谷歌、脸书等IT巨头的发力下，自然语言处理里面的人机对话、自动翻译也取得了长远进步。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、深度学习关键术语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之前的课程中，我们介绍了感知器、多层感知器。\n",
    "![perceptron](pic/perceptron.png)\n",
    "\n",
    "上图是基本的感知器，这里面只有输入层和输出层，输入和输出的关系为：\n",
    "\n",
    "$$ y = \\phi(wx+b) = sign(wx+b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中基本感知器单元（神经元）可以简化为下图：\n",
    "\n",
    "![neuralnetwork](pic/neuralnetwork.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个神经元对输入数据进行权重求和，然后进行激活再输出。**注意，神经元里面都是有求和和激活两个过程，如果只有求和而没有激活的过程，那么无论网络有多少层，最终都是线性模型。** 激活函数的功能就是赋予神经网络非线性性，以此能够构造任意函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络可以看成是多层感知器堆砌而成，其中前面一层的输出作为后面层的输入。\n",
    "\n",
    "![neuralnetwork](pic/neuralnetworks.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.权重\n",
    "\n",
    "神经元对于输入数据先进行加权求和然后进行激活，前后层神经元之间的连接都有权重（表示为$w$)，**权重就是深度学习中所需要求解的变量**，一般可以通过梯度下降的方法迭代求解。如果前层神经元有m个，后层神经元有n个，那么两层之间全连接权重数目为$m*n$（不考虑偏置b）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.激活函数\n",
    "常用的激活函数有sigmoid, softmax, relu以及tanh。\n",
    "- sigmoid。Sigmoid 函数的值域为 [0,1]，表达式为$g(x)=\\sigma (x)=\\frac {1}{1+e^{-x}}$。该激活函数如今并不常用，因为它的梯度太容易饱和，不过 二分类、RNN-LSTM 网络如今还会需要用到它。\n",
    "![sigmoid](http://img.blog.csdn.net/20160917153242569)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- softmax。多分类的问题经常使用softmax来将最终的输出归一化，可以看作是sigmoid的拓展，将输出结果归一化。假设输出结果是个k维向量$z=(z_1, z_2,..., z_k)$，那么:\n",
    "$$y = softmax(z)= \\frac {(e^{z_i}, e^{z_2}, ..., e^{z_k})} { \\sum_{i=1}^k{e^{z_i}} }$$\n",
    "\n",
    "- Relu。在卷积神经网络中使用较多。\n",
    "![relu](http://img.blog.csdn.net/20160917151806843)\n",
    "\n",
    "- tanh。Tanh函数值范围[-1, 1]， 表达式:$tanh(x) = \\frac {1-e^{-2x}}{1+e^{2x}}$。tanh 激活函数因为 ReLU 函数的普及使用而不那么流行了。\n",
    "![tanh](http://img.blog.csdn.net/20160917150431945)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.损失函数\n",
    "\n",
    "与机器学习中损失函数类似，分为回归和分类两大类。\n",
    "- 回归问题损失函数：均方差$\\sum_{i=1}^{N} (y_i - \\hat y_i)^2/N$、平均绝对误差$\\sum_{i=1}^{N} \\mid y_i - \\hat y_i \\mid/N$，这里面的y对应是最终标签值，比如房价、股票价格。\n",
    "\n",
    "- 分类问题损失函数：\n",
    "交叉熵(cross-entropy)$$l = \\sum_{i=1}^{N} y_i.* log( \\hat y_i)/N$$\n",
    "注意这里面的$y_i$是独热编码(one-hot encoder)。比如4分类问题中，当前$y_i=[0, 0, 0, 1]$，而预测的softmax结果是$\\hat y_i=[0.2, 0.2, 0.2, 0.4]$，则交叉熵为\n",
    "$$l_i = 0*log(0.2)+0*log(0.2)+0*log(0.2)+1*log(0.4)=log(0.4)=-0.92$$\n",
    "理想的情况是正确的类别预测为1，其他为0，最终损失函数为0.\n",
    "\n",
    "注意，损失函数需要具有可导性，方便计算梯度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.防止过拟合方法\n",
    "通常深度学习训练的网络结构复杂、参数众多，过拟合是不得不考虑的问题。一种方法是增大数据量，但是带有标签的数据很金贵，不是很容易获取；一种方法是数据加强，利用现有的数据进行变化获取新的数据，比如图像处理中，给图形加上噪声、白化、扭曲、旋转构造新的图作为样本；还有一种方法正则化，将权重的平方和作为目标函数的一部分；除此之外，目前应用的比较广泛的丢弃(dropout)方法。\n",
    ">开篇明义，dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。\n",
    ">http://blog.csdn.net/stdcoutzyx/article/details/49022443\n",
    ">![dropout](pic/n7-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5.组归一化(batch normalization)\n",
    "在随机梯度下降过程中时，对每层神经元输入值进行归一化，使得均值为0，方差为1。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6初始化\n",
    "\n",
    "深度学习模型训练中，需要先将每层权重参数进行初始化然后通过输入值计算输出值。最简单的初始化就是将所以权重和偏置设置为0，但是这种方法效果很差。初始化方式对训练最终结果很有影响，常用的初始化方法有随机初始化（比如生成正态分布的随机值），还要最近比较热门的[Xavier](http://blog.csdn.net/shuzfan/article/details/51338178)初始化方法（一种综合考虑前后层神经元个数的均匀分布），初始化权重使得每一层输出的方差尽量相等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.梯度下降算法\n",
    "初始化权重后，通过反馈神经网络计算权重的梯度值，然后利用梯度下降算法进行迭代求解权重。\n",
    "\n",
    "$$W = W - \\alpha * \\Delta W$$\n",
    "\n",
    "常用的梯度下降算法有：\n",
    "- BGD：每次训练时，将所以的样本数据输入进行计算。但是通常样本量非常大，导致计算缓慢，内存占用严重。\n",
    "- SGD：每层训练时，随机选取一小部分样本进行计算，比如选取32个样本进行迭代计算求解梯度。由于时抽取样本，得到的梯度肯定有误差.因此学习速率需要逐渐减小。梯度具有噪声。\n",
    "- Momentum：动量的原理，定义了一个速度变量，开始时的梯度对速度影响比较大，后面的梯度对其影响小，理论上可以收敛。具体请参考[CSDN博客](http://blog.csdn.net/u014595019/article/details/52989301)。\n",
    "- RMSProp：RMSProp通过引入一个衰减系数，让r每回合都衰减一定比例，类似于Momentum中的做法。\n",
    "- Adam：Adam(Adaptive Moment Estimation)本质上是带有动量项的RMSprop，它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。具体请参考[CSDN博客](http://blog.csdn.net/u014595019/article/details/52989301)。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 二、前向神经网络和BP算法 \n",
    "\n",
    "神经网络在类型上可分为前向、反馈、随机、竞争四个类型，我们谈论的神经网络大部分属于前向神经网络(feed forward neural networks)。前向神经网络有以下特点（参考[斯坦福课件](http://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/Architecture/feedforward.html))：\n",
    "- 神经元按层聚集, 第一层输入，最后一层输出. 中间层与外界没有练习, 称之为隐层。\n",
    "- 每层的神经元与下一层的神经元都相连接，因此“信息”从一层“前向”传递到下一层。\n",
    "- 同一层的神经元之间无连接。\n",
    "![fullyconnected](pic/neuralnetwork3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### BP算法\n",
    "BP全称为Back Propagation，意思为反向传播，该方法是用来对人工神经网络进行优化的，即误差反向传播算法。 \n",
    "它属于监督学习方式，上面我们对照上图介绍两个过程：\n",
    "- 正向传播。输入信号从输入层经隐含层，传至输出层的过程，给定输入和每层的权重，我们可以逐步计算出各层的输出和最终的输出。层与层之间神经元连接都有权重W，比如中间层节第1个点与输入层x1节点的权重为$W_{11}^{(1)}$，与输入层x2节点与连接权重是$W_{12}^{(1)}$。上面$a_i^{(l)}$表示等l层的第i个节点的输出，比如$a_1^{(2)}$表示第2层第1个节点的输出值，其中f(x)为激活函数，这里是sigmoid函数。为表达方便，我们将线性相加部分定为z，将激活后的输出定为a：\n",
    "\n",
    "$$ z_1^{(2)} = W_{11}^{(1)}x_1 + W_{12}^{(1)}x_2 + W_{13}^{(1)}x_3+ b_1^{(1)}$$\n",
    "\n",
    "$$ a_1^{(2)} = f(z_1^{(2)})$$\n",
    "\n",
    "$$ z_2^{(2)} = W_{21}^{(1)}x_1 + W_{22}^{(1)}x_2 + W_{23}^{(1)}x_3+ b_2^{(1)}$$\n",
    "\n",
    "$$ a_2^{(2)} = f(z_2^{(2)})$$\n",
    "\n",
    "$$ z_3^{(2)} = W_{31}^{(1)}x_1 + W_{32}^{(1)}x_2 + W_{33}^{(1)}x_3+ b_3^{(1)}$$\n",
    "\n",
    "$$ a_3^{(2)} = f(z_3^{(2)})$$\n",
    "\n",
    "$$ z_1^{(3)} = W_{11}^{(2)}a_1^{(2)} + W_{12}^{(2)}a_2^{(2)} + W_{13}^{(2)}a_3^{(2)}+ b_1^{(2)}$$\n",
    "\n",
    "$$ \\hat y =h(x)= a_1^{(3)} = f(z_1^{(3)})$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 反向误差传播。将误差从输出层反向传至输入层，并通过梯度下降算法来调节连接权值与偏置值的过程。需要计算目标损失函数对权重W和偏置b的梯度然后进行迭代求解。有很多[参考资料](http://blog.csdn.net/heyongluoyao8/article/details/48213345)介绍反向误差传播，但是因为变量比较多，看上去比较复杂。下面就是结合上图进行逐步计算。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前向神经网络计算完毕的情况下，假设对任意单个样本x，y损失函数为：\n",
    "\n",
    "$$ L = (y-\\hat y)^2/2=(y-h(x))^2$$\n",
    "\n",
    "首先求解第2层和第3层之间的全连接权重偏导数：\n",
    "$$ \\frac {\\partial L}{\\partial W_{ij}^{(2)}} = \\frac {\\partial L}{\\partial z_i^{(3)}} * \\frac {\\partial z_i^{(3)}}{\\partial W_{ij}^{(2)}} =\\frac {\\partial L}{\\partial h} * \\frac {\\partial h}{\\partial z_i^{(3)}} * \\frac {\\partial z_i^{(3)}}{\\partial W_{ij}^{(2)}}$$\n",
    "\n",
    "其中：\n",
    "\n",
    "$$ \\frac {\\partial L}{\\partial h} = - (y - h) $$\n",
    "\n",
    "$$ \\frac {\\partial h}{\\partial z_1^{(3)}} =  h * (1 - h)$$\n",
    "\n",
    "$$ \\frac {\\partial z_1^{(3)}}{\\partial W_{1j}} = a_j^{(2)}$$\n",
    "\n",
    "所以第2、3层连接权重梯度（这里只有1个输出，所以i=1）：\n",
    "\n",
    "$$ \\frac {\\partial L}{\\partial W_{1j}^{(2)}} = - (y - h) * h * (1 - h) * a_j^{(2)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了表达的方便，假设\n",
    "$$\\delta_i^{(3)} = \\frac {\\partial L}{\\partial z_i^{(3)}}$$\n",
    "那么\n",
    "\n",
    "$$ \\frac {\\partial L}{\\partial W_{ij}^{(2)}} = \\delta_i^{(3)} * a_j^{(2)}$$\n",
    "\n",
    "下面计算第1、2层之间的连接权重梯度：\n",
    "\n",
    "$$ \\frac {\\partial L}{\\partial W_{ij}^{(1)}} = \\frac {\\partial L}{\\partial z_1^{(3)}} * \\frac {\\partial z_1^{(3)}}{\\partial z_i^{(2)}} * \\frac {\\partial z_i^{(2)}}{\\partial W_{ij}^{(1)}}$$\n",
    "\n",
    "其中$$\\frac {\\partial z_1^{(3)}}{\\partial z_i^{(2)}} = \\frac {\\partial W_{1i}a_i^{(2)}}{\\partial z_i^{(2)}} = W_{1i}^{(2)} * a_i^{(2)}*(1- a_i^{(2)})$$\n",
    "\n",
    "$$\\frac {\\partial z_i^{(2)}}{\\partial W_{ij}^{(2)}} = x_j$$\n",
    "\n",
    "所以：\n",
    "$$\\delta_i^{(2)} = \\frac {\\partial L}{\\partial z_i^{(2)}} = \\delta_i^{(3)} * W_{1i}^{(2)} * a_i^{(2)}*(1- a_i^{(2)})$$\n",
    "\n",
    "$$\\frac {\\partial L}{\\partial W_{ij}^{(1)}} = \\delta_i^{(2)} * x_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是针对每一个样本损失函数求的梯度，实际上要将所有样本损失函数求均值再算梯度。误差反向传播主要使用了是链式求导法则。\n",
    "![bp](pic/bp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义,$W_{ji}^{(t)}$表示第t层第i个神经元与第(t+1)层第j个神经元之间的权重，$z_{j}^{(t+1)}$表示第(t+1)层第j个神经元的线性输入，$a_{j}^{(t+1)}$表示第(t+1)层第j个神经元激活后的输出，$n_{t+2}$表示第(t+2)层的神经元个数。在上面推导中，为了简化表达，我们可以定义：\n",
    "\n",
    "$$z_j^{(t+1)}=\\sum_{i=1}^{n_t} W_{ji}^{(t)}a_{i}^{(t)}$$\n",
    "\n",
    "$$a_j^{(t+1)}=f(z_j^{(t+1)})，f为激活函数$$\n",
    "\n",
    "$$\\delta_{j}^{t}=\\frac {\\partial L}{\\partial z_j^{t}}，为了简化表达的中间量$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_k^{(t+2)}=\\sum_{j=1}^{n_{t+1}}W_{kj}^{(t+1)}a_j^{(t+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们要求求解梯度：\n",
    "$$\\frac {\\partial L}{\\partial W_{ji}^{(t)}}=\\frac {\\partial L}{\\partial z_j^{(t+1)}} * \\frac {\\partial z_j^{(t+1)}}{\\partial W_{ji}{(t)}}=\\delta_j^{(t+1)}*a_i^{(t)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\delta_j^{(t+1)}=\\frac {\\partial L}{\\partial z_j^{(t+1)}}=\\frac {\\partial L}{\\partial a_j^{(t+1)}}* \\frac {\\partial a_j^{(t+1)}}{\\partial z_j^{(t+1)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于(t+2)层神经元来讲，每个线性和$z_k^{(t+2)}$都是$ a_j^{(t+1)}$的函数：\n",
    "$$\\frac {\\partial L}{\\partial a_j^{(t+1)}}= \\sum_{k=1}^{n_{t+2}} {\\frac {\\partial L}{\\partial z_k^{(t+2)}}}*{ \\frac {\\partial  z_k^{(t+2)}}{\\partial a_j^{(t+1)}}}=\\sum_{k=1}^{n_{t+2}} {\\delta_k^{(t+2)}}*W_{kj}^{(t+1)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\delta_j^{(t+1)}=\\sum_{k=1}^{n_{t+2}} ({\\delta_k^{(t+2)}}W_{kj}^{(t+1)})*f'(z_j^{(t+1)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们推导出了第(t+1)层变量$\\delta_j^{(t+1)}$与第(t+2)层变量$\\delta_k^{(t+2)}$之间的关系，我们可以一直递推下去，从而计算出所有的权重梯度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、深度学习架构\n",
    "\n",
    "### 1.全连接神经网络\n",
    "\n",
    "任意两层直接的神经元之间都有连接，如下图：\n",
    "![fullyconnected](pic/digits.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面提到过，每两层之间的全连接权重数量等于两层的神经元个数之积，如果层数比较多，神经元数量比较多，那么权重变量个数可能会是个天文数字，无论是对样本数量还是计算资源都有很高的要求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.卷积神经网络\n",
    "\n",
    "卷积是信号处理中重要的概念，可以方便信号从时域到频域的计算。卷积神经网络的具体分析可以看知乎博客：https://zhuanlan.zhihu.com/p/25249694 。\n",
    "\n",
    "$$c(x,y)=\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty}f(s,t)\\times g(x-s,y-t) \\ ds \\ dt$$\n",
    "\n",
    "深度学习中，卷积神经网络实现的是共享权重，通过卷积核与输入特征多次进行卷积计算提取特征。下图是一维卷积，卷积核上面的数值就是权重，卷积核每次移动一步对输入信号局部进行运算输出特征。\n",
    "![conv](pic/conv1.png)\n",
    "如果是全连接的话，需要35个权重变量（不计偏置），如果使用卷积模型的话，只有3个权重值，减少了权重变量数目，但是增大了计算量。\n",
    "![conv](pic/conv2.png)\n",
    "下图是二维卷积：\n",
    "![conv](pic/Convolution_schematic.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图，假设左边的是```5*5```的图片，黄色区域是```3*3```的卷积核，卷积核权重{1，0，1；0，1，0；1，0，1}。卷积核在图像上以单步步长滑动，每次计算时，卷积核对覆盖的```3*3```区域进行矩阵点乘计算，最后输出的是```3*3```矩阵。\n",
    "\n",
    "假设输入图片的像素为$H * W$，卷积核为$F*F$，每次移动的步长为$S*S$，无填充（padding=0），那么输出特征维数：\n",
    "$$ 高度：(H-F)/S+1 ，\\ 宽度： (W-F)/S + 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "跟卷积一起使用的技术有池化（pooling），比如max_pooling，整个图片被不重叠的分割成若干个同样大小的小块（pooling size）。每个小块内只取最大的数字，再舍弃其他节点后，保持原有的平面结构得出 output。：\n",
    "![max_pooling](pic/pool.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络在图像处理中取得了突出的效果，成为当前计算机图像中不可或缺的神器。下面是2012年ImageNet大赛中一举夺冠的AlexNet：\n",
    "![AlexNet](http://img.blog.csdn.net/20160101171955054)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.循环神经网络(Recurrent Neural Networks)\n",
    "\n",
    "循环神经网络特点在于神经元存在环路，也就是前一时刻的输出作为当前时刻的输入，适合序列类型的数据，比如时间序列、文本，在自然语言处理中有广泛的应用，下面的图来自[CSDN博客](http://blog.csdn.net/heyongluoyao8/article/details/48636251)。\n",
    "![rnn](http://img.blog.csdn.net/20150921225622105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "从上图可以看出，当前t时刻神经元状态值为：\n",
    "$$s_t = f(Ux_t + Ws_{t-1}),其中f一般是非线性的激活函数，如tanh或ReLU$$\n",
    "输出值为：\n",
    "$$O_t = softmax(Vs_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "误差为所有时刻的误差和，那么损失函数对权重的偏导数为：\n",
    "$$\\frac {\\partial E}{\\partial W} = \\sum_{t=1}^T \\frac {\\partial E_t}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t时刻的状态与前面时刻的状态都有关系，通过全导数公式对t时刻梯度计算为：\n",
    "\n",
    "$$\\frac {\\partial E_t}{\\partial W} = \\sum_{k=1}^t \\frac {\\partial E_t}{\\partial O_t} *  \\frac {\\partial O_t}{\\partial s_t} * \\frac {\\partial s_t}{\\partial s_k} * \\frac {\\partial s_k}{\\partial W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意其中有一项前后状态之间的偏导数，当k=1时：\n",
    "\n",
    "$$\\frac {\\partial s_t}{\\partial s_1}=\\frac {\\partial s_t}{\\partial s_{t-1}} * \\frac {\\partial s_{t-1}}{\\partial s_{t-2}} *...*\\frac {\\partial s_2}{\\partial s_1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac {\\partial s_t}{\\partial s_{t-1}}=W^T*diag(f'(Ux_t+Ws_{t-1}))$$\n",
    "\n",
    "$$\\frac {\\partial s_t}{\\partial s_1}= \\prod_{j=2}^t W^T*diag(f'(Ux_j+Ws_{j-1}))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当t很大时，$W^T$要连乘很多次，如果W有大于1的特征值，梯度将会变很大；如果有小于1的特征值，梯度将会衰减近乎0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "循环神经网络可以记录历史信息，在自然语言处理中使用比较广泛，比如自动翻译，记忆网络等。但是，上图vanilla RNN存在梯度萎缩和梯度爆炸的问题，所以学者们又提出了两种RNN变种：\n",
    "- LSTM(Long short term memory)\n",
    "![RNN](http://img.blog.csdn.net/20150921230954367)\n",
    "![RNN](http://img.blog.csdn.net/20150921231113097)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs与一般的RNNs结构本质上并没有什么不同，只是使用了不同的函数去去计算隐藏层的状态。在LSTMs中，i结构被称为cells，可以把cells看作是黑盒用以保存当前输入xt之前的保存的状态ht−1，这些cells更加一定的条件决定哪些cell抑制哪些cell兴奋。它们结合前面的状态、当前的记忆与当前的输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GRU(Gated Recurrent Unit)\n",
    " \n",
    "![gru](http://img.blog.csdn.net/20150921230927637)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRUs也是一般的RNNs的改良版本，主要是从以下两个方面进行改进。一是，序列中不同的位置处的单词(已单词举例)对当前的隐藏层的状态的影响不同，越前面的影响越小，即每个前面状态对当前的影响进行了距离加权，距离越远，权值越小。二是，在产生误差error时，误差可能是由某一个或者几个单词而引发的，所以应当仅仅对对应的单词weight进行更新。GRUs的结构如下图所示。GRUs首先根据当前输入单词向量word vector已经前一个隐藏层的状态hidden state计算出update gate和reset gate。再根据reset gate、当前word vector以及前一个hidden state计算新的记忆单元内容(new memory content)。当reset gate为1的时候，new memory content忽略之前的所有memory content，最终的memory是之前的hidden state与new memory content的结合。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 四、深度学习工具\n",
    "\n",
    "随着深度学习的迅猛发展，各大企业及科研机构争先推出深度学习工具，力图打造有利于自身长远发展的生态圈。当前主流的深度学习工具包基本上都提供Python接口，下面罗列了常用的几个工具包：\n",
    "- tensorflow，谷歌，支持GPU和分布式计算，可以实现跨平台应用，编码相对复杂。\n",
    "- keras，深度学习的高级API，底层运行在Theano上或tensorflow，类似scikit-learn风格，简单明了。\n",
    "- Pytorch，facebook今年推出来的神器，类似于Numpy风格，支持GPU，编码比tensorflow简单，效率也很高，但是功能上还与tensorflow有差距。\n",
    "- MxNet，Amazon力推，计算效率高。\n",
    "- Caffe2，图像处理行业使用较多。\n",
    "- Paddle，百度出品。\n",
    "- Theano，蒙特利尔大学。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、深度学习进阶资料\n",
    "\n",
    "1. 斯坦福大学，卷积神经网络和视觉识别课程，http://cs231n.stanford.edu/ 。\n",
    "2. 斯坦福大学，深度学习与自然语言处理， http://cs224d.stanford.edu/ 。\n",
    "3. 牛津大学， 深度学习与自然语言处理， https://github.com/oxford-cs-deepnlp-2017/lectures 。\n",
    "4. 优达学城， 人工智能纳米学位，https://cn.udacity.com/ai/ 。\n",
    "5. 优达学城， 自动驾驶纳米学位， https://cn.udacity.com/drive 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
