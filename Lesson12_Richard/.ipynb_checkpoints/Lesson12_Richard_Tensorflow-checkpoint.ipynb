{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面的课程中我们介绍了深度学习的基本概念，大家也阅读了相当的深度学习资料，本次课主要介绍深度学习工具以及应用。当前流行的深度学习框架有下列：\n",
    "- Tensorflow，谷歌公司开源工具包，https://tensorflow.google.cn/\n",
    "- Pytorch，脸书公司开源工具包，http://pytorch.org/\n",
    "- Caffe, Caffe2，加州伯克利大学，第一个主流的工业级深度学习工具,http://caffe.berkeleyvision.org/\n",
    "- Mxnet，亚马逊，https://mxnet.incubator.apache.org/index.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "根据github上访问量进行统计排名，tensorflow目前热度最高。\n",
    "<img src=http://xilinx.eetrend.com/files-eetrend-xilinx/article/201705/11406-29989-githubzhongzuishouhuanyingdekaiyuanshenduxuexikuangjiapaiming.png width=\"50%\" height=\"50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面对几个主流的框架进行比较,具体参考[csdn博客](http://blog.csdn.net/myarrow/article/details/52064608)：\n",
    "![comparison](pic/comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选用tensorflow的原因：\n",
    "- Python接口\n",
    "- 跨平台，支持windows, Linux, MacOS\n",
    "- 可移动开发，支持Android\n",
    "- 自动求导，定义好损失函数后，tf自动进行求导计算\n",
    "- 可视化（tensorboard）\n",
    "- 庞大社群支持\n",
    "- 免费\n",
    "\n",
    "tensorflow的缺点：\n",
    "- TensorFlow 的每个计算流都必须构造为一个静态图（比如说要预先定义好变量的尺寸大小），且缺乏符号性循环（symbolic loops），这会带来一些计算困难。\n",
    "- 没有对视频识别很有用的三维卷积（3-D convolution）。\n",
    "- 尽管 TensorFlow 现在比起始版本（v0.5）快了 58 倍，，但在执行性能方面依然落后于竞争对手。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综合考虑，tensorflow是个比较适中的开源工具。tensorflow有比较强大的社区，无论是英文的官网还是中文的csdn博客都有相当多的资料可供学习：\n",
    "- 深度学习基石，cn.udacity.com\n",
    "- 官方网站，tensorflow.google.cn\n",
    "- github示例， github.com/tensorflow/models\n",
    "- 斯坦福大学课程，http://web.stanford.edu/class/cs20si/index.html\n",
    "\n",
    "TF升级比较频繁，书籍很容易过时，建议直接查看官方网站教程和示例。tensorflow支持高级接口keras，keras风格类似于scikit-learn，简单易学，能够让学者集中精力调试模型本身而不用花费大量时间编写代码，有兴趣可以访问keras.io 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先运行下简单的tensorflow程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#创建数据流图\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #定义常量和变量\n",
    "    a = tf.constant(3)\n",
    "    b = tf.constant(5)\n",
    "    w = tf.constant(2)\n",
    "    #定义计算关系\n",
    "    c = w*a + b\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里程序没有立即进行计算并给出结果，需要创造会话运行然后返回结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "#创建会话\n",
    "sess = tf.Session(graph=graph)\n",
    "print(sess.run(c))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "#对比Python自带的变量\n",
    "a = 3\n",
    "b = 5\n",
    "w = 2\n",
    "c = w*a + b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1.数据流图与会话\n",
    "\n",
    "TensorFlow™ 是一个使用数据流图进行数值计算的开源软件库。图中的节点代表数学运算， 而图中的边则代表在这些节点之间传递的多维数组（张量）。这种灵活的架构可让您使用一个 API 将计算工作部署到桌面设备、服务器或者移动设备中的一个或多个 CPU 或 GPU。 TensorFlow 最初是由 Google 机器智能研究部门的 Google Brain 团队中的研究人员和工程师开发的，用于进行机器学习和深度神经网络研究， 但它是一个非常基础的系统，因此也可以应用于众多其他领域【引用自tensorflow.google.cn】。\n",
    "\n",
    "首先介绍下张量：\n",
    "- 可以是标量，如1，2，3\n",
    "- 可以是向量，[1, 2, 3]\n",
    "- 可以是矩阵，[[1,2,3],[4,5,6]]\n",
    "在tensorflow中，常量、变量、占位符都是张量。\n",
    "\n",
    "运行tensorflow程序需要两个步骤：\n",
    "- 创建数据流图\n",
    "- 运行工作流图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 3]\n",
      " [4 5]]\n",
      "[[0 2]\n",
      " [4 6]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    a = tf.constant([2, 2], name=\"a\")\n",
    "    b = tf.constant([[0, 1], [2, 3]], name=\"b\")\n",
    "    x = tf.add(a, b, name=\"add\")\n",
    "    y = tf.multiply(a, b, name=\"multiply\")\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    x, y = sess.run([x, y])\n",
    "    print(x)\n",
    "    print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 数据流图(Data Flow Graph)\n",
    "用于定义数据张量(tensor)，数据间的运算关系，但是注意计算具有延迟性，具体的计算需要在会话里完成。数据流图包括：\n",
    "### 1.1.1 定义张量，用来定义输入输出变量，参数变量，比如示例中的a,b,w\n",
    "- 常量，tf.constant定义的张量，训练中不变化，比如a = tf.constant([2, 2], name=\"a\")\n",
    "- 变量，比如tf.Variable定义的张量（权重、偏置等），训练过程中发生变化，变量有两种定义：\n",
    " - tf.Variable\n",
    " - tf.get_variable\n",
    "- 占位符，tf.placeholder定义的张量，占位符并没有初始值，它只会分配必要的内存。在会话中，占位符可以使用 feed_dict 馈送数据，比如输入的数据和标签，然后将训练或测试数据传给占位符进行运算。Python的链表，Numpy数据不能直接作为tensorflow的输入，通过占位符可以进行转换。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 定义计算关系\n",
    "张量直接之间的计算关系，比如c=w*a+b\n",
    "- 一种方法是直接使用“+、-、*、/”等运算符\n",
    "```\n",
    "a = constant(1)\n",
    "b = constant(2)\n",
    "w = constant(3)\n",
    "c = a*w+b\n",
    "```\n",
    "\n",
    "- 一种是利用tensorflow所提供的运算函数，此方法便于定义计算节点的属性。\n",
    "```\n",
    "a = constant(1)\n",
    "b = constant(2)\n",
    "w = constant(3)\n",
    "c = tf.add(tf.mutiply(a, w), b)\n",
    "```\n",
    "\n",
    "### 1.1.3 定义损失函数\n",
    "\n",
    "对于分类问题常用的损失函数就是交叉熵\n",
    "$$loss = \\sum_{i=1}^N y_i.*log(\\hat y_i)$$\n",
    "可以自己通过公式计算，也可以使用现成函数tf.nn.softmax_cross_entropy_with_logits(logits, labels,dim=-1, name=None)。\n",
    "对于回归问题可以使用最小均方差作为损失函数。\n",
    "### 1.1.4 定义最优化方法\n",
    "常用的最优化方法是随机梯度下降算法(Stochastic Descending Gradient)，如果不想调试很多参数而又想获得一个比较好的结果可以使用Adam方法，这些概念再之前课程中有所介绍，也可以观看吴恩达博士的深度学习课程讲解。\n",
    "\n",
    "这些最优化方法是通过迭代求解权重、偏置等参数。\n",
    "![dataflowgraph](http://www.tensorfly.cn/images/tensors_flowing.gif)\n",
    "\n",
    "## 1.2 会话(Session)\n",
    "会话中运行数据流图，比如实现变量初始化，完成计算并输出结果。**从上面可以看出，如果我们想要利用tensorflow实现某种模型算法，必须要对该模型算法原理以及实现流程掌握清楚，比如从变量定义到损失函数选择到梯度下降算法设计。**如果只想利用现成的深度学习模型参加竞赛、应用解决某个问题，可以使用编码更为高效的Keras。\n",
    "\n",
    "## 1.3 简单示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 读取MNIST数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Import required packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "#Load MNIST dataset\n",
    "import tensorflow.examples.tutorials.mnist.input_data as input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_tr = mnist.train.images.shape[0]# number of training samples\n",
    "n_ts = mnist.test.images.shape[0]#number of testing samples\n",
    "n_pixel = mnist.train.images.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27752dc49e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mnist.train.images[5].reshape([28, 28]))\n",
    "print(mnist.train.labels[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 构建数据流图"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们设计一个简单的线性判别模型，可以说是多分类逻辑回归问题。公式表达如下：\n",
    "$$ Z = XW + b$$\n",
    "$$ A = softmax(Z)$$\n",
    "$$ \\hat Y = log(A)$$\n",
    "$$ Loss = Y .* \\hat Y$$\n",
    "\n",
    "输入为X、Y，参数为W、b，Z、A、$\\hat Y$为中间变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a linear graph, define its architecture in terms of nodes and compuatations\n",
    "graph_linear = tf.Graph()\n",
    "#Set it as default graph\n",
    "with graph_linear.as_default():\n",
    "    #Define input placeholder and target placeholder\n",
    "    #which can be fed data later\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x_placeholder')\n",
    "    #One hot-encoder\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], name='y_placeholder')\n",
    "    #Define a linear model structure\n",
    "    #Create weights and biases\n",
    "    W = tf.Variable(tf.zeros([784,10]), name='weights')\n",
    "    b = tf.Variable(tf.zeros([10]), name='biases')\n",
    "    #Softmax output \n",
    "    y = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "    #Cross entropy as the loss\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Use gradient descent method to optmize\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Softmax:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面建好的数据流图是个静态图，后面我们通过对占位符喂数据进行训练，每训练一次，tensorflow都会利用指定的梯度下降算法更新参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy： 0.2063\n",
      "Test Accuracy： 0.9087\n",
      "Test Accuracy： 0.9061\n",
      "Test Accuracy： 0.9101\n",
      "Test Accuracy： 0.9148\n",
      "Test Accuracy： 0.9145\n",
      "Test Accuracy： 0.9217\n",
      "Test Accuracy： 0.9195\n",
      "Test Accuracy： 0.9204\n",
      "Test Accuracy： 0.9219\n",
      "Test Accuracy： 0.9226\n",
      "Test Accuracy： 0.922\n",
      "Test Accuracy： 0.9232\n",
      "Test Accuracy： 0.9193\n",
      "Test Accuracy： 0.9164\n",
      "Test Accuracy： 0.9214\n",
      "Test Accuracy： 0.9241\n",
      "Test Accuracy： 0.9185\n",
      "Test Accuracy： 0.923\n",
      "Test Accuracy： 0.9225\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 10\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session, in order to run the graph\n",
    "with tf.Session(graph=graph_linear) as sess:\n",
    "    #Initialize all the variables created above\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for _ in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            #Feed the dataset to the placeholders     \n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            #Pass the data to a dictionary\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "            #Train the model by feeding batch data\n",
    "            _, loss = sess.run([train_step, cross_entropy], feed_dict=feed_dict)\n",
    "            #Calculate acccuracy after 500 times training\n",
    "            if step%500 == 0:\n",
    "                result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                print(\"Test Accuracy：\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体的tensorboard图如下：\n",
    "![Linear_Graph](pic/linear_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2.Tensorflow利器\n",
    "\n",
    "## 2.1可视化tensorboard\n",
    "注意tensorboard文件路径里面不能含有中文，否则可视化服务找不到事件文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a linear graph, define its architecture in terms of nodes and compuatations\n",
    "graph_linear = tf.Graph()\n",
    "#Set it as default graph\n",
    "with graph_linear.as_default():\n",
    "    #Define input placeholder and target placeholder\n",
    "    #which can be fed data later\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x_placeholder')\n",
    "    #One hot-encoder\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], name='y_placeholder')\n",
    "    #Define a linear model structure\n",
    "    #Create weights and biases\n",
    "    W = tf.Variable(tf.zeros([784,10]), name='weights')\n",
    "    b = tf.Variable(tf.zeros([10]), name='biases')\n",
    "    #Softmax output \n",
    "    z = tf.matmul(x,W) + b\n",
    "    y = tf.nn.softmax(z)\n",
    "    #**Add the value to tensorboard**\n",
    "    tf.summary.histogram('y',y);\n",
    "    #Cross entropy as the loss\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Add the value to the tensorboard\n",
    "    tf.summary.scalar('loss_function', cross_entropy)\n",
    "    #Use gradient descent method to optmize\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    #Create summary writer\n",
    "    #if tf.gfile.Exists(\"/temp/mnist_logs\"):\n",
    "        #tf.gfile.DeleteRecursively(\"/temp/mnist_logs\");\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('temp/mnist_logs',graph_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy： 0.1861\n",
      "Test Accuracy： 0.9059\n",
      "Test Accuracy： 0.9141\n",
      "Test Accuracy： 0.9189\n",
      "Test Accuracy： 0.9155\n",
      "Test Accuracy： 0.8968\n",
      "Test Accuracy： 0.914\n",
      "Test Accuracy： 0.9185\n",
      "Test Accuracy： 0.9037\n",
      "Test Accuracy： 0.919\n",
      "Test Accuracy： 0.9172\n",
      "Test Accuracy： 0.9138\n",
      "Test Accuracy： 0.9226\n",
      "Test Accuracy： 0.9191\n",
      "Test Accuracy： 0.9207\n",
      "Test Accuracy： 0.9227\n",
      "Test Accuracy： 0.9202\n",
      "Test Accuracy： 0.919\n",
      "Test Accuracy： 0.919\n",
      "Test Accuracy： 0.9154\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 10\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session, in order to run the graph\n",
    "with tf.Session(graph=graph_linear) as sess:\n",
    "    #Initialize all the variables created above\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            #Feed the dataset to the placeholders     \n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            #Pass the data to a dictionary\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "            #Train the model by feeding batch data\n",
    "            _, loss, summary_str  = sess.run([train_step, cross_entropy, merged_summary_op], feed_dict=feed_dict)\n",
    "            #Calculate acccuracy after 500 times training\n",
    "            if step%500 == 0:\n",
    "                result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                print(\"Test Accuracy：\", result)\n",
    "        summary_writer.add_summary(summary_str, i);\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2命名空间\n",
    "当电脑里面文件过多时，我们可以通过创建不同的文件夹将类似的文件放到一起，使得查找更为便捷。同样的，在tensorflow中，我们可以使用命名空间，来将不同的变量放到不同的区域模块中，甚至通过设置命名空间的属性来实现参数共享。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "命名空间可以通过tf.name_scope和tf.variable_scope来实现，其中tf.name_scope针对tf.Variable方法所创建的变量进行分区，而tf.variable_scope针对tf.get_variable所创建的变量进行分区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable_6:0' shape=() dtype=int32_ref>\n",
      "<tf.Variable 'Variable_7:0' shape=() dtype=int32_ref>\n"
     ]
    }
   ],
   "source": [
    "#没有命名空间的变量\n",
    "a = tf.Variable(0)\n",
    "b = tf.Variable(0)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'name_scope_9/Variable:0' shape=() dtype=int32_ref>\n",
      "<tf.Variable 'name_scope_9/Variable_1:0' shape=() dtype=int32_ref>\n",
      "<tf.Variable 'variable_scope/c:0' shape=() dtype=float32_ref>\n",
      "Tensor(\"name_scope_9/Add:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('name_scope'):\n",
    "    a = tf.Variable(0)\n",
    "    b = tf.Variable(0)\n",
    "    #c = tf.get_variable(shape=(), name='c')\n",
    "    z = tf.add(a, b)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'variable_scope/Variable:0' shape=() dtype=int32_ref>\n",
      "<tf.Variable 'variable_scope/Variable_1:0' shape=() dtype=int32_ref>\n",
      "<tf.Variable 'variable_scope/c:0' shape=() dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('variable_scope'):\n",
    "    a = tf.Variable(0)\n",
    "    b = tf.Variable(0)\n",
    "    c = tf.get_variable(shape=(), name='c')\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a linear graph, define its architecture in terms of nodes and compuatations\n",
    "graph_linear = tf.Graph()\n",
    "#Set it as default graph\n",
    "with graph_linear.as_default():\n",
    "    #Define input placeholder and target placeholder\n",
    "    #which can be fed data later\n",
    "    with tf.name_scope('InputTensor'):\n",
    "        x = tf.placeholder(tf.float32, [None, 784], name='x_placeholder')\n",
    "        #One hot-encoder\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10], name='y_placeholder')\n",
    "    with tf.name_scope('Parameters'):\n",
    "        #Define a linear model structure\n",
    "        #Create weights and biases\n",
    "        W = tf.Variable(tf.zeros([784,10]), name='weights')\n",
    "        b = tf.Variable(tf.zeros([10]), name='biases')\n",
    "    with tf.name_scope('LinearModel'):\n",
    "        #Softmax output \n",
    "        z = tf.matmul(x,W) + b\n",
    "        y = tf.nn.softmax(z)\n",
    "        #**Add the value to tensorboard**\n",
    "        tf.summary.histogram('y',y);\n",
    "        #Cross entropy as the loss\n",
    "        cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "        #Add the value to the tensorboard\n",
    "        tf.summary.scalar('loss_function', cross_entropy)\n",
    "    with tf.name_scope('Optimizer'):\n",
    "        #Use gradient descent method to optmize\n",
    "        train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    with tf.name_scope('Prediction'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    #Create summary writer\n",
    "    #if tf.gfile.Exists(\"/temp/mnist_logs\"):\n",
    "        #tf.gfile.DeleteRecursively(\"/temp/mnist_logs\");\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('temp/mnist_logs',graph_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy： 0.3313\n",
      "Test Accuracy： 0.9128\n",
      "Test Accuracy： 0.9145\n",
      "Test Accuracy： 0.9166\n",
      "Test Accuracy： 0.9146\n",
      "Test Accuracy： 0.9118\n",
      "Test Accuracy： 0.9054\n",
      "Test Accuracy： 0.9184\n",
      "Test Accuracy： 0.9171\n",
      "Test Accuracy： 0.9179\n",
      "Test Accuracy： 0.9126\n",
      "Test Accuracy： 0.9224\n",
      "Test Accuracy： 0.9136\n",
      "Test Accuracy： 0.9194\n",
      "Test Accuracy： 0.9215\n",
      "Test Accuracy： 0.9224\n",
      "Test Accuracy： 0.9222\n",
      "Test Accuracy： 0.9192\n",
      "Test Accuracy： 0.9225\n",
      "Test Accuracy： 0.9182\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 10\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session, in order to run the graph\n",
    "with tf.Session(graph=graph_linear) as sess:\n",
    "    #Initialize all the variables created above\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            #Feed the dataset to the placeholders     \n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            #Pass the data to a dictionary\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "            #Train the model by feeding batch data\n",
    "            _, loss, summary_str  = sess.run([train_step, cross_entropy, merged_summary_op], feed_dict=feed_dict)\n",
    "            #Calculate acccuracy after 500 times training\n",
    "            if step%500 == 0:\n",
    "                result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                print(\"Test Accuracy：\", result)\n",
    "        summary_writer.add_summary(summary_str, i);\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.2 模型保存与恢复\n",
    "\n",
    "当前我们的深度学习训练都是在内存中完成，如果在梯度下降算法迭代过程中出现异常，往往需要重新从头进行训练，将会耗费大量时间，如果能够将训练中间的状态进行存储，能够极大方便后续的训练，特别是对于数据量比较大、参数比较多的模型，如果在之前训练较好的模型基础上进行训练，那么可以节省训练时间进一步提升效益。Tensorflow可以将训练的结果如参数值存储在本地，方便后续的测试和迁移学习。\n",
    "\n",
    "模型的保存使用tf.train.Savor命令，模型的恢复使用saver.restore()命令。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a linear graph, define its architecture in terms of nodes and compuatations\n",
    "graph_linear = tf.Graph()\n",
    "#Set it as default graph\n",
    "with graph_linear.as_default():\n",
    "    #Define input placeholder and target placeholder\n",
    "    #which can be fed data later\n",
    "    x = tf.placeholder(tf.float32, [None, 784], name='x_placeholder')\n",
    "    #One hot-encoder\n",
    "    y_ = tf.placeholder(tf.float32, [None, 10], name='y_placeholder')\n",
    "    #Define a linear model structure\n",
    "    #Create weights and biases\n",
    "    W = tf.Variable(tf.zeros([784,10]), name='weights')\n",
    "    b = tf.Variable(tf.zeros([10]), name='biases')\n",
    "    #Softmax output \n",
    "    z = tf.matmul(x,W) + b\n",
    "    y = tf.nn.softmax(z)\n",
    "    #**Add the value to tensorboard**\n",
    "    tf.summary.histogram('y',y);\n",
    "    #Cross entropy as the loss\n",
    "    cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "    #Add the value to the tensorboard\n",
    "    tf.summary.scalar('loss_function', cross_entropy)\n",
    "    #Use gradient descent method to optmize\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    #Create summary writer\n",
    "    #if tf.gfile.Exists(\"/temp/mnist_logs\"):\n",
    "        #tf.gfile.DeleteRecursively(\"/temp/mnist_logs\");\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    saver=tf.train.Saver()\n",
    "\n",
    "summary_writer = tf.summary.FileWriter('temp/mnist_logs',graph_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy： 0.1187\n",
      "Test Accuracy： 0.9138\n",
      "Test Accuracy： 0.9097\n",
      "Test Accuracy： 0.9121\n",
      "Test Accuracy： 0.912\n",
      "Test Accuracy： 0.9191\n",
      "Test Accuracy： 0.9186\n",
      "Test Accuracy： 0.9174\n",
      "Test Accuracy： 0.9178\n",
      "Test Accuracy： 0.9204\n",
      "Test Accuracy： 0.9136\n",
      "Test Accuracy： 0.9157\n",
      "Test Accuracy： 0.9228\n",
      "Test Accuracy： 0.9191\n",
      "Test Accuracy： 0.9139\n",
      "Test Accuracy： 0.9188\n",
      "Test Accuracy： 0.9138\n",
      "Test Accuracy： 0.9224\n",
      "Test Accuracy： 0.9227\n",
      "Test Accuracy： 0.9223\n"
     ]
    }
   ],
   "source": [
    "e_pochs = 10\n",
    "batch_size = 64\n",
    "num_steps = int(n_tr/batch_size)\n",
    "#Create a session, in order to run the graph\n",
    "with tf.Session(graph=graph_linear) as sess:\n",
    "    #Initialize all the variables created above\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    for i in range(e_pochs):\n",
    "        for step in range(num_steps):\n",
    "            #Feed the dataset to the placeholders     \n",
    "            batch_data, batch_labels = mnist.train.next_batch(batch_size)\n",
    "            #Pass the data to a dictionary\n",
    "            feed_dict = {x : batch_data, y_ : batch_labels}\n",
    "            #Train the model by feeding batch data\n",
    "            _, loss, summary_str  = sess.run([train_step, cross_entropy, merged_summary_op], feed_dict=feed_dict)\n",
    "            #Calculate acccuracy after 500 times training\n",
    "            if step%500 == 0:\n",
    "                result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "                print(\"Test Accuracy：\", result)\n",
    "        #Save the model for each epoch\n",
    "        saver.save(sess,'model/model.ckpt', global_step=i+1)\n",
    "        #Save the summary in order to display them in tensorboard\n",
    "        summary_writer.add_summary(summary_str, i);\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model/model.ckpt-10\n",
      "Test Accuracy： 0.9139\n"
     ]
    }
   ],
   "source": [
    "#Test the model\n",
    "with tf.Session(graph=graph_linear) as sess:\n",
    "    #Get the latest model\n",
    "    model_file=tf.train.latest_checkpoint('model/')\n",
    "    #Pass the saved model parameters to sess\n",
    "    #saver.restore(sess, \"/model/model.ckpt9\")\n",
    "    saver.restore(sess,model_file)\n",
    "    result = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
    "    print(\"Test Accuracy：\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
