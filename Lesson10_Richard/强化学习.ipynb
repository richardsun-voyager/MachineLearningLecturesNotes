{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习也是机器学习里面一个重要的方向，机器学习分为监督学习、非监督学习，还有就是强化学习。所谓强化学习就是智能系统从环境到行为映射的学习，以使奖励信号(强化信号)函数值最大，也就是说给定环境，选择合适的行为使得最终奖励信号值最大，在智能控制机器人及分析预测等领域有许多应用【引用自[百度百科](https://baike.baidu.com/item/强化学习/2971075?fr=aladdin)】，比如挑战围棋高手李世石、柯洁成功的阿尔法狗，训练机器人玩游戏，无人驾驶等方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "对于我们来讲，可以从简单电脑游戏入手强化学习，利用程序教电脑玩游戏。Elon Musk建立了openai.com公司，旨在开发开源的人工智能项目，强化学习是其中一部分, 比如 [OpenAI Gym](https://github.com/openai/gym)模块，可以用来训练玩游戏。\n",
    "\n",
    "\n",
    "强化学习入门资料比较多，除了优达学城自带课程外，若有兴趣可以参考学习：\n",
    "\n",
    "Google Deepmind大牛David Silver的强化学习课程：http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf\n",
    "\n",
    "卡耐基梅隆大学课程：https://www.cmucoursefind.xyz/courses/10-703/s17\n",
    "\n",
    "加州伯克利大学课程：http://rll.berkeley.edu/deeprlcourse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![reinforcement_learning-w150](pic\\rl.png)\n",
    "\n",
    "上图引用自David Silver[课件](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 示例\n",
    "![maze](pic/maze.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设这是一个机器人走迷宫的游戏(引用自David Silver[课件](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/intro_RL.pdf))，目的是从起点(Start)走到终点(Goal)。迷宫中白色的部分是可以走的通道，黑色的部分是障碍。我们可以把整个迷宫看作是坐标系中的格子，机器人有传感器可以观察获取自己当前的位置信息，但是它不知道怎样的路线是正确的，只要靠尝不停尝试；机器人每次可以执行一次动作，即向东南西北任一方向迈进一步；机器人每执行一次行动则会收到一个奖励信号，如果成功到达目的则获得额外奖励。\n",
    "\n",
    "# 术语\n",
    "\n",
    "可以看出，强化学习涉及到一系列的决策行动，每次行动对最终结果都有影响。在强化学习中，我们将决策、行动执行实体称为Agent，比如上图中走迷宫的机器人；每次行动称为Action，比如上图中机器人可以执行4种行动；奖励称为Reward；状态称为State，比如机器人的坐标；环境称为Environment，如上图的迷宫；决策的准则称为Policy，如上图中机器人通过当前信息决策采取何种行动。\n",
    "\n",
    "- 实体与环境(agent and environment):实体执行动作，接受观测结果，接受奖励值；环境接受实体的动作，发布观测值，发布奖励。\n",
    "- 历史和状态(history and state):历史就是到当前时刻为止实体的观察值、动作、奖励，状态就是历史信息的函数。这里，我们可以认为，状态信息就是观测值，实体的状态也等价于环境的状态。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 原理\n",
    "\n",
    "强化学习作为一个序列决策（Sequential Decision Making）问题，它需要连续选择一些行为，从这些行为完成后得到最大的收益作为最好的结果。它在没有任何label告诉算法应该怎么做的情况下，通过先尝试做出一些行为——然后得到一个结果，通过判断这个结果是对还是错来对之前的行为进行反馈。由这个反馈来调整之前的行为，通过不断的调整算法能够学习到在什么样的情况下选择什么样的行为可以得到最好的结果【[David Silver](https://www.leiphone.com/news/201608/If3hZy8sLqbn6uvo.html)】。\n",
    "\n",
    "## 强化学习与监督学习区别\n",
    "\n",
    "- 监督学习：\n",
    " - 独立的样本，学习过程中每个样本对其它样本没有影响，比如每个样本的分类和回归结\n",
    " 果只由自身特征决定\n",
    " - 每个样本有着明确的输出标签\n",
    " - 能够实时对学习好坏进行反馈\n",
    " - 目标函数由输出结果和标签决定\n",
    " \n",
    "- 强化学习：\n",
    " - 序列决策，连续选择一些行为，每个行为对后面有影响\n",
    " - 没有label告诉每步行为的对错\n",
    " - 结果反馈有延时， 走了很多步之后才知道之前选择的好坏\n",
    " - 目标函数是累积奖励信号之和\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "强化学习与监督学习有不少区别，从前文中可以看到监督学习是有一个label（标记）的，这个label告诉算法什么样的输入对应着什么样的输出。而强化学习没有label告诉它在某种情况下应该做出什么样的行为，只有一个做出一系列行为后最终反馈回来的reward signal，这个signal能判断当前选择的行为是好是坏。另外强化学习的结果反馈有延时，有时候可能需要走了很多步以后才知道之前某步的选择是好还是坏，而监督学习如果做了比较坏的选择则会立刻反馈给算法。强化学习面对的输入总是在变化，不像监督学习中——输入是独立分布的。每当算法做出一个行为，它就影响了下一次决策的输入。强化学习和标准的监督式学习之间的区别在于，它并不需要出现正确的输入/输出对，也不需要精确校正次优化的行为。强化学习更加专注于在线规划，需要在Exploration（探索未知的领域）和Exploitation（利用现有知识）之间找到平衡。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 要素\n",
    "\n",
    "一个agent由Policy（策略）、Value function（价值函数）、Model（模型）三部分组成，但这三部分不是必须同时存在的。\n",
    "\n",
    "- Policy（策略）：它根据当前看到的observation来决定action，是从state到action的映射。\n",
    "有两种表达形式:\n",
    " - 一种是Deterministic policy（确定策略）即a=π(s)，在某种状态s下，一定会执行某个动作a。\n",
    " - 一种是Stochastic policy（随机策略）即$π(a|s)=p[A_t=a|S_t=s]$，它是在某种状态下执行某个动作的概率，比如说，上图机器人在迷宫中不论什么位置，其向东南西北迈进的概率都是25%。\n",
    " ![policy](pic/policy.png)\n",
    "\n",
    "\n",
    "- Value function（价值函数）：\n",
    " - 它预测了当前状态下未来可能获得的reward的期望。\n",
    " - 用于衡量当前状态的好坏。\n",
    " - 表达式可以写成$V_π(s)=E_π[R_{t+1}+rR_{t+2}+r^2R_{t+3}…|St=s]$。\n",
    " ![value](pic/value.png)\n",
    "\n",
    "\n",
    "- Model（模型）：预测environment下一步会做出什么样的改变，从而预测agent接收到的状态或者reward是什么。因而有两种类型的model：\n",
    " - 一种是预测下一个state的transition model即$P^a_{ss′}=p[S_{t+1}=s′|S_t=s,A_t=a]$\n",
    " - 一种是预测下一次reward的reward model即$R^a_s=E[R_{t+1}|S_t=s,A_t=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 强化学习求解模型\n",
    "\n",
    "强化学习的目的是求解策略选择一系列动作，使得目标函数（奖励值加权和）最大。求解的方法很多，这里简单介绍下Q-learning，网上有个比较经典的例子：http://blog.csdn.net/pi9nc/article/details/27649323 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning(维基百科https://en.wikipedia.org/wiki/Q-learning )：\n",
    "\n",
    "> Q-learning是一种无模型（model free)强化学习方。特别是，Q-learning能够用来为任何给定有限的马尔可夫过程查找最优化策略，选择每步动作。它是通过学习“动作-价值”函数，最终通过给定的状态给出每个动作所产生的预期价值。策略就是实体根据给定状态值来选择动作。如果有了“动作-价值”函数，每次都可以根据当前状态选择最优动作来获取最优策略。Q-learning的优点是不需要环境模型，此外它还能处理随机转换和奖励情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提到Q-learning，我们需要先了解Q的含义。\n",
    "> Q为动作效用函数（action-utility function），用于评价在特定状态下采取某个动作的优劣，可以将之理解为实体（Agent，我们聪明的机器人）的大脑。我们可以把Q当做是一张表。表中的每一行是一个状态，每一列（这个问题中共有4列）表示一个动作（东南西北）。\n",
    "\n",
    "\n",
    "| state(location)        | E           | W  | S  | N |\n",
    "| ------------- |:-------------:| -----:| -----:| -----:|\n",
    "| (1, 3)    | 2 | 3 |3 |3 |\n",
    "| (2, 4)     | 1      |   1 |3 |3 |\n",
    "| (5, 6) | 1      |    1 |3 |3 |\n",
    "\n",
    "\n",
    "> 作者：牛阿\n",
    "链接：https://www.zhihu.com/question/26408259/answer/123230350\n",
    "来源：知乎\n",
    "著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning中重要的一部分就是求解Q矩阵，也就是“状态-动作-价值”矩阵，找到了这个矩阵，就可以通过当前状态来选择使得效用最大的动作。Q矩阵迭代求解公式【引用于[维基百科](https://en.wikipedia.org/wiki/Q-learning)】：\n",
    "![Q-learning](https://wikimedia.org/api/rest_v1/media/math/render/svg/390d024c2ee2a2c2f709642401a3a7b44f7b2e4e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\gamma$  是介于0到1之间的数 $(\n",
    "0\\leq \\gamma \\leq 1)$， 称为 折扣系数， 用来平衡近期和远期奖励值重要性。$\\alpha$是学习速率。$Q(s_t, a_t)$是Q矩阵对应的效用值，$s_t$是当前状态，$a_t$是当前的动作，$r_t$是当前的奖励值， $s_{t+1}$是采取当前动作后的下一个状态。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据公式可以看出，学习速率$\\alpha$越大，保留之前训练的效果就越少。折扣因子$\\gamma$越大，所起到的作用就越大。但指什么呢？机器人在对状态进行更新时，会考虑到眼前利益$r_t$，和记忆中的利益$max_a{Q(s_{t+1}, a)}$。$max_a{Q(s_{t+1}, a)}$指的便是记忆中的利益,它是指实体记忆里下一个状态的动作中效用值的最大值。如果走迷宫的机器人之前在下一个状态的某个动作上吃过甜头（选择了某个动作之后获得了1的奖赏），那么它就更希望提早地得知这个消息，以便下回在状态可以通过选择正确的动作继续进入这个吃甜头的状态。\n",
    "\n",
    "作者：牛阿\n",
    "链接：https://www.zhihu.com/question/26408259/answer/123230350\n",
    "来源：知乎\n",
    "著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "迭代算法伪码如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "Initialize Q arbitrarily //随机初始化Q值\n",
    "Repeat (for each episode): //每一次游戏，从机器人出发到目的地是一个episode\n",
    "    Initialize S //机器人出发，S为初始位置的状态\n",
    "    Repeat (for each step of episode):\n",
    "        根据当前Q和位置S，使用一种策略，得到动作A //这个策略可以是ε-greedy等\n",
    "        做了动作A，机器人到达新的位置S'，并获得奖励R //奖励可以是-1或+1\n",
    "        Q(S,A) ← (1-α)*Q(S,A) + α*[R + γ*maxQ(S',a)] //在Q中更新S\n",
    "        S ← S'\n",
    "    until S is terminal //即到机器人到达目的位置\n",
    "\n",
    "作者：牛阿\n",
    "链接：https://www.zhihu.com/question/26408259/answer/123230350\n",
    "来源：知乎\n",
    "著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 探索和利用（Exploitation VS Exploration）\n",
    "> 强化学习是一种试错(trial-and-error)的学习方式：最开始的时候不清楚environment（环境）的工作方式，不清楚执行什么样的action（行为）是对的，什么样的action（行为）是错的。因而agent需要从不断尝试的经验中发现一个好的policy，从而在这个过程中获取更多的reward。\n",
    "在学习过程中，会有一个在Exploration（探索）和Exploitation（利用）之间的权衡。\n",
    "引用：https://www.leiphone.com/news/201608/If3hZy8sLqbn6uvo.html\n",
    "\n",
    "在Q-learning模型中，我们的策略是每次根据Q矩阵选择产生价值最大的动作，实际上，这也是充分利用利用之前状态中所学到的信息，称之为exploitation。同时有个疑问，每次实体都采取当前状态效用值最大的动作（比如，机器人走迷宫时在某一步往南走得到了正奖励，那么他会记住这个甜头下一次到了这个位置继续往南走），那会不会有更好的选择一直没有被探索到，导致最终目标值收敛于局部最优值。怎么样解决，于是就有人提出了ε-greedy方法，即每个状态有ε的概率进行探索（上例中即随机选取东南西北），随机尝试一些新的动作，而剩下的1-ε的概率则利用Q矩阵进行选择（选取当前状态下效用值较大的那个动作）。ε一般取值较小，0.01即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
